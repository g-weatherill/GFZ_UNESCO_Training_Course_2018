{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Recurrence (how to infer long-term rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most critical components of any seismic hazard analysis is an estimate of the long-term rates of seismicity from your region or seismogenic source. \n",
    "\n",
    "So ... the questions we will try to answer are:<b>\n",
    "\n",
    "1. For a given region how often could we expect to see earthquakes larger than a certain magnitude?\n",
    "\n",
    "2. What is the probability of observing this earthquake in a 50 year period?</b>\n",
    "\n",
    "Arguably the most well-established observations of seismology is the <b>Gutenberg & Richter (1944)</b> model which describes the cumulative rate of events above a given magnitude ($N_C$) to the magnitude itself via:\n",
    "\n",
    "$\\log_{10} N_C = a - b \\cdot m$\n",
    "\n",
    "Theoretically this simple linear relation is easy to fit to earthquake data, but there is a complication! As we saw in the completeness notebook, it is common to have unequal periods of observational completeness across the entire magnitude range. If we don't correct for this somehow then our estimates will be biased.\n",
    "\n",
    "This notebook will show you how to do this using the magnitude and time data in the completeness notebook. We will consider here two methods (both of which have precedent for use in real applications):\n",
    "\n",
    "1. Weighted Least Squares\n",
    "\n",
    "2. Weichert (1980) Maximum Likelihood\n",
    "\n",
    "Let's get started ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Out usual numerical and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# A function to find the root of an equation - we'll come to this later!\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "# Copied across from the Completeness notebook!\n",
    "def plot_magnitude_time_density(magnitudes, years, mbins, time_bins,\n",
    "                                completeness_table=[], vmin=1, vmax=100,\n",
    "                                filename=None, filetype=\"png\", dpi=300):\n",
    "    \"\"\"\n",
    "    Create a magnitude density plot\n",
    "    :param magnitudes:\n",
    "        Vector of magnitudes\n",
    "    :param years:\n",
    "        Vector of years\n",
    "    :param mbins:\n",
    "        Edges of the magnitude bins\n",
    "    :param time_bins:\n",
    "        Edges of the time bins\n",
    "    :param completeness_table:\n",
    "        If present, the table of completeness\n",
    "    \"\"\"\n",
    "    # Generate a 2-D historgram in terms of magnitude and time\n",
    "    counter = np.histogram2d(years, magnitudes, bins=(time_bins,mbins))[0]\n",
    "    # Plot the density\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.pcolormesh(time_bins[:-1], mbins[:-1],\n",
    "                   counter.T, norm=LogNorm(vmin, vmax))\n",
    "    # Add axes and labels\n",
    "    plt.xlabel(\"Year\", fontsize=16)\n",
    "    plt.ylabel(\"Magnitude\", fontsize=16)\n",
    "    plt.tick_params(labelsize=14)\n",
    "    plt.colorbar()\n",
    "    plt.grid()\n",
    "    # If a completeness table is given add on a step line\n",
    "    if len(completeness_table):\n",
    "        completeness = np.array(completeness_table)\n",
    "        plt.step(completeness[:, 0], completeness[:, 1], \"k--\", lw=2)\n",
    "    # If the filename is specified then export to file \n",
    "    if filename:\n",
    "        plt.savefig(filename, format=filetype, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the catalogue we used previously in the completeness notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the catalogue from the completeness notebook\n",
    "catalogue = np.genfromtxt(\"./test_completeness_catalogue_1.csv\", delimiter=\",\", skip_header=1)\n",
    "# Sort into it's magnitudes and years\n",
    "magnitudes = catalogue[:, 0]\n",
    "years = catalogue[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the same completeness table you found in the completeness notebook ...\n",
    "completeness_table = [[2005, 3.0],\n",
    "                      [2005, 3.5],\n",
    "                      [1990, 4.0],\n",
    "                      [1975, 4.5],\n",
    "                      [1960, 5.0],\n",
    "                      [1950, 5.5],\n",
    "                      [1900, 6.0],\n",
    "                      [1900, 6.5]]\n",
    "# ... and turn it into a numpy array\n",
    "completeness_table = np.array(completeness_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we did before, let's take a look at the density of events\n",
    "magnitude_bins = np.arange(3.0, 7.1, 0.1)\n",
    "year_bins = np.arange(1900., 2016., 1)\n",
    "plot_magnitude_time_density(magnitudes, years,\n",
    "                            mbins=magnitude_bins,\n",
    "                            time_bins=year_bins,\n",
    "                            completeness_table=completeness_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to the main work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core principal behind the methods we will look at are that we need to find the best estimate of the rate of events in different magnitude bins. <i>However</i> we can only make this estimation by adjusting for the period of time for which we believe that bin is <i>complete</i>.\n",
    "\n",
    "In this example, we use our completeness table to define the bins for magnitude, which gives the following as the bins:\n",
    "\n",
    "`3.0 <= M < 3.5`\n",
    "\n",
    "`3.5 <= M < 4.0`\n",
    "\n",
    "`4.0 <= M < 4.5`\n",
    "\n",
    "`4.5 <= M < 5.0`\n",
    "\n",
    "`5.0 <= M < 5.5`\n",
    "\n",
    "`5.5 <= M < 6.0`\n",
    "\n",
    "`6.0 <= M < 6.5`\n",
    "\n",
    "`6.5 <= M < 7.0`\n",
    "\n",
    "So, we need to count in each bin the number of earthquakes within the completeness period and the duration of the completeness period\n",
    "\n",
    "Pay attention step-by-step here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our catalogue ends at the end of 2015\n",
    "end_year = 2015.\n",
    "# Number of bins is taken from the number of rows in the completeness table\n",
    "nbins = completeness_table.shape[0]\n",
    "# The width of the bins is 0.5 M\n",
    "bin_width = 0.5\n",
    "# We will count the number of earthquakes, rates, midpoints of the bins and duration - so start with zeros\n",
    "n_mags = np.zeros(nbins)\n",
    "rates = np.zeros_like(n_mags)\n",
    "midpoints = np.zeros_like(n_mags)\n",
    "duration = np.zeros_like(n_mags)\n",
    "# Loop through each row in the completeness table\n",
    "for i, row in enumerate(completeness_table):\n",
    "    # Starting year is the completeness year\n",
    "    start_year = row[0]\n",
    "    # Duration is the time between the end of the catalogue (incl. 2015) and\n",
    "    # and the year of completeness \n",
    "    duration[i] = end_year - start_year + 1\n",
    "    # A bit of logical work entering here, select the years >= the completeness year\n",
    "    selected_years = years >= start_year\n",
    "    # Define the upper and lower magnitude bounds of the bin\n",
    "    mlow = row[1]\n",
    "    mhigh = row[1] + bin_width\n",
    "    midpoints[i] = (mlow + mhigh) / 2.\n",
    "    # From the selected years find which earthquakes are within the magnitude range\n",
    "    selected_magnitudes = np.logical_and(\n",
    "        magnitudes[selected_years] >= mlow,\n",
    "        magnitudes[selected_years] < mhigh\n",
    "    )\n",
    "    # Count the number of earthquakes selected\n",
    "    n_mags[i] = np.sum(selected_magnitudes)\n",
    "    # Divide by the duration of completeness for the magnitude bin\n",
    "    rates[i] = float(n_mags[i]) / duration[i]\n",
    "\n",
    "    \n",
    "# Let's take a look at what we've just counted\n",
    "print(\"Mlow   Mmid   Mhigh   Comp. Year  Num.Events   Rate (/yr)\")\n",
    "for i in range(nbins):\n",
    "    print(\"%.1f     %.1f   %.2f          %.0f         %3g   %.6f\" % (\n",
    "        completeness_table[i, 1],\n",
    "        completeness_table[i, 1] + bin_width,\n",
    "        completeness_table[i, 1] + (bin_width / 2.),\n",
    "        completeness_table[i, 0],\n",
    "        n_mags[i],\n",
    "        rates[i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also plot a figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(completeness_table[:, 1], rates, 0.5, log=True)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Magnitude\", fontsize=14)\n",
    "plt.ylabel(\"Normalised Rate /yr\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the cumulative rates greater than or equal to each magnitude bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cumulative rates\n",
    "cum_rates = np.zeros_like(n_mags)\n",
    "for i in range(nbins):\n",
    "    # Sum the rates greater than or equal to each bin\n",
    "    cum_rates[i] = np.sum(rates[i:])\n",
    "    \n",
    "# Add this information onto the table\n",
    "print(\"Mlow   Mmid   Mhigh   Comp. Year  Num.Events   Rate (/yr)   Cum. Rate\")\n",
    "for i in range(nbins):\n",
    "    print(\"%.1f     %.1f   %.2f          %.0f         %3g   %8.6f   %8.6f\" %(\n",
    "        completeness_table[i, 1],\n",
    "        completeness_table[i, 1] + bin_width,\n",
    "        completeness_table[i, 1] + (bin_width / 2.),\n",
    "        completeness_table[i, 0],\n",
    "        n_mags[i],\n",
    "        rates[i],\n",
    "        cum_rates[i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(completeness_table[:, 1], rates, 0.5, log=True)\n",
    "plt.plot(midpoints, rates, \"ko-\", lw=2, label=\"incremental\")\n",
    "plt.plot(midpoints, cum_rates, \"rs-\", lw=2, label=\"cumulative\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Magnitude\", fontsize=14)\n",
    "plt.ylabel(\"Normalised Rate /yr\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gutenberg-Richter Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data we can see that once we account for completeness in the catalogue, the relation between magnitude and the common logarithm of the <i>cumulative</i> number of events per year greater than that magnitude is (approximately) linear:\n",
    "\n",
    "$\\log_{10} \\left( {Nc} \\right) = a - bM$\n",
    "\n",
    "Can we fit the a- and b-value?\n",
    "\n",
    "Well let's start by looking at the data in the common logarithmic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(midpoints, np.log10(rates), \"ko-\", lw=2, label=\"incremental\")\n",
    "plt.plot(midpoints, np.log10(cum_rates), \"rs-\", lw=2, label=\"cumulative\")\n",
    "plt.xlabel(\"Magnitude\", fontsize=14)\n",
    "plt.ylabel(r\"$\\log_{10} \\left( {N_C} \\right)$\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a straight line fit could work!\n",
    "\n",
    "Let's do simple linear regression using a tool from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces two outputs - the best fitting parameters and the covariance matrix\n",
    "best_fit, C = np.polyfit(midpoints, # x-values, use the midpoints of the magnitude bin\n",
    "                         np.log10(cum_rates),  # y-values, use the log10 of the cumulative rates\n",
    "                         deg=1, # order of polynomial - in this case linear (order = 1)\n",
    "                         cov=True)  # Also return the covariance matrix\n",
    "\n",
    "# The variance of the best fitting parameters can be found on the leading diagonal of the\n",
    "# covarince matrix. Take the square root of these to find the standard deviations\n",
    "uncertainties = np.sqrt(np.diag(C))\n",
    "# Best-fit is an array with the [b-value, a-value]\n",
    "bls, als = best_fit\n",
    "# b-value is negative in this case, so turn it positive\n",
    "bls = np.fabs(bls)\n",
    "print(\"a = %.4f (+/- %.4f)  b = %.4f (+/- %.4f)\" %\n",
    "      (als, uncertainties[1], bls, uncertainties[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have our Gutenberg-Richter model, right? $\\log_{10} \\left( {Nc} \\right) = a - bM$\n",
    "\n",
    "Well, yes. But ask yourself the following ...\n",
    "\n",
    "1. How confident are we that the rates we observe in each bin are representative of the long term rates for that bin? How many earthquakes do we have in each bin?\n",
    "\n",
    "2. If we are fitting to the cumulative rates, is the rate of events with M > 5 an observation independent of those rates with M > 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Weichert (1980) Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Don't worry too much about the mathematics, this is just here for completeness!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weichert (1980) proposes a maximum likelihood approach for finding parameters of the Gutenberg-Richter model for binned magnitudes of time-varying completeness. See the paper for the full derivation of the model.\n",
    "\n",
    "Firstly, there is some calculus involved here, so we switch from the common logarithm to the natural logarithm.\n",
    "\n",
    "$N_C = 10^{a - b \\cdot M} = e^{\\alpha - \\beta \\cdot M}$\n",
    "\n",
    "where $\\alpha = a\\cdot\\ln\\left( {10} \\right)$ and $\\beta = b\\cdot\\ln\\left( {10} \\right)$\n",
    "\n",
    "The likelihood function for the estimation of $\\beta$ is found to be:\n",
    "\n",
    "$L \\left( {\\beta | n_i, m_i, t_i} \\right) = \\frac{N!}{\\prod_i n_i !}\\cdot\\prod_i \\left( {\\frac{t_i \\exp\\left( {-\\beta m_i} \\right)}{\\sum_j t_j \\exp \\left( {-\\beta m_j} \\right)}} \\right) ^{n_i}$\n",
    "\n",
    "where $n_i$ is the number of earthquakes in magnitude bin $m_i$, with the completeness duration $t_i$, and $N$ is the total number of earthquakes <i>within the period of completeness</i>.\n",
    "\n",
    "\n",
    "This has a maximum at $\\ln L$ when:\n",
    "\n",
    "$\\frac{\\sum_i t_i m_i \\exp \\left( {-\\beta m_i} \\right)}{\\sum_j t_j \\exp \\left( {-\\beta m_j} \\right)} - \\frac{\\sum n_i m_i}{N} = 0$\n",
    "\n",
    "\n",
    "The value for $\\beta$ can be solved by iteration or Newton conjugate-gradient method (as below). Once this is known the rate of earthquakes above the minimum magnitude $m_{min}$ is determined from:\n",
    "\n",
    "$N_C \\left( {m \\geq m_{min}} \\right) = N \\cdot \\frac{\\sum_i \\exp\\left( {-\\beta m_i} \\right)}{\\sum_j t_j \\exp\\left( {-\\beta m_j} \\right)}$\n",
    "\n",
    "From this we can infer a-value from the original form of the G-R model:\n",
    "\n",
    "$a = \\log_{10} \\left( {N_C \\left[{m \\geq m_{min}}\\right]} \\right) + b \\cdot m_{min}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines our likelihood function\n",
    "def likelihood(beta, mags, nmags, duration):\n",
    "    \"\"\"\n",
    "    Returns the minimum of the log-likelihood value. An optimum bvalue should bring\n",
    "    this close to 0.0\n",
    "    :param bvalue:\n",
    "        b-value\n",
    "    :param mags:\n",
    "        mid-points of the magnitude bins\n",
    "    :param nmags:\n",
    "        Number of events in the specific magnitude bins\n",
    "    :param duration:\n",
    "        Completeness durations (in years) of the magnitude bins\n",
    "    \"\"\"\n",
    "    mbar = np.sum(nmags * mags) / float(np.sum(nmags))\n",
    "    numerator = np.sum(duration * mags * np.exp(-beta * mags))\n",
    "    denominator = np.sum(duration * np.exp(-beta * mags))\n",
    "    value = (numerator / denominator) - mbar\n",
    "    return np.fabs(value)\n",
    "\n",
    "# This applies the Weichert method of maximum likelihood estimation\n",
    "def weichert(mags, nmags, duration, b0=1.0):\n",
    "    \"\"\"\n",
    "    Implements the Weichert maximum likelihood estimator of rate and\n",
    "    b-value for catalogues with time-varying completeness\n",
    "    :param mags:\n",
    "        mid-points of the magnitude bins\n",
    "    :param nmags:\n",
    "        Number of events in the specific magnitude bins\n",
    "    :param duration:\n",
    "        Completeness durations (in years) of the magnitude bins\n",
    "    :param b0:\n",
    "        Initial guess of b-value (optional, default = 1)\n",
    "    :returns:\n",
    "        a-value, b-value, stddev_a, stddev_b, rate_m0, stddev_rate_m0\n",
    "    \"\"\"\n",
    "    # Initial guess at b-value, turn to beta\n",
    "    beta0 = b0 * np.log(10.)\n",
    "\n",
    "    # Find the optimum b-value such that the loglikelihood is equal to 0\n",
    "    # For this we use scipy's fsolve function\n",
    "    [beta] = fsolve(likelihood, x0=beta0, args=(mags, nmags, duration))\n",
    "    b = beta / np.log(10.)\n",
    "\n",
    "    # Now we have the b-value, need to get a-value\n",
    "    # Get total number of earthquakes\n",
    "    neq = float(np.sum(nmags))\n",
    "    \n",
    "    # We see this exp(-beta * m) being used a lot, so just calculate once and\n",
    "    # save as a variable\n",
    "    e_beta_m = np.exp(-beta * mags)\n",
    "    \n",
    "    # Get the rate of events above Mmin\n",
    "    rate_m0 = neq * np.sum(e_beta_m) / np.sum(duration * e_beta_m)\n",
    "\n",
    "    # Get the a-value from G-R: a = Nc + b * mmin\n",
    "    a = np.log10(rate_m0) + b * mags[0]\n",
    "\n",
    "    # Get the variance of b (from equation 9 of Weichert, 1980)\n",
    "    var_beta = ((np.sum(duration * e_beta_m)) ** 2.) /\\\n",
    "        ((np.sum(duration * mags * e_beta_m) ** 2.) -\n",
    "         (np.sum(duration * e_beta_m) * np.sum(duration * (mags ** 2.) * e_beta_m)))\n",
    "    var_beta = (-1.0 / neq) * var_beta\n",
    "    stddev_b = np.sqrt(var_beta) / np.log(10.)\n",
    "\n",
    "    # Get the variance of rate >= M0\n",
    "    stddev_rate_m0 = np.sqrt(rate_m0 / neq)\n",
    "    return a, b, stddev_b, rate_m0, stddev_rate_m0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the Weichert function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awc, bwc, sigma_bwc, rate_m0, stddev_rate_m0 = weichert(midpoints,\n",
    "                                                        n_mags.astype(float),\n",
    "                                                        duration)\n",
    "print(\"a = %.4f  b = %.4f (+/- %.4f)  Rate M >= %.2f is %.3f (+/- %.4f))\" % \n",
    "      (awc, bwc, sigma_bwc, completeness_table[0, 1], rate_m0, stddev_rate_m0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two different estimates of a- and b-value and they seem to agree with each other!\n",
    "\n",
    "So, let's take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(midpoints, np.log10(cum_rates), \"o\", label=\"observation\")\n",
    "plt.plot(midpoints, awc - bwc * midpoints, \"k-\", lw=2, label=\"Weichert\")\n",
    "plt.plot(midpoints, als - bls * midpoints, \"r-\", lw=2, label=\"Linear LSQ.\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"M\", fontsize=14)\n",
    "plt.ylabel(r\"$\\log_{10} N_C$\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we now answer the questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the a- and b-values we found (don't worry about uncertainties right now) ...\n",
    "\n",
    "How often should we expect an earthquake of magnitude greater than or equal to 6.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_M6above = 10.0 ** (awc - bwc * 6.0)\n",
    "print(\"The rate of events above magnitude 6 is %.3f per year\" % rate_M6above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end of the story?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite ...\n",
    "\n",
    "Theoretically the Gutenberg-Richter model is unbounded, but we have seen in the lectures that is not really the case. In practice we use a form of the model that places an upper bound ($M_{MAX}$) and a lower bound ($M_{MIN}$) on the relation.\n",
    "\n",
    "Again, we will skip the derivation, but the final model is:\n",
    "\n",
    "$\\lambda_m = \\nu \\frac{\\exp\\left[ {-\\beta\\left( {m - m_{min}}\\right)}\\right] - \\exp\\left[ {-\\beta\\left( {m_{max} - m_{min}}\\right)}\\right]}{1 - \\exp\\left[ {-\\beta\\left( {m_{max} - m_{min}}\\right)}\\right]}$\n",
    "\n",
    "where $\\nu$ is the rate of events above $M_{min}$ and $\\beta = b\\cdot \\ln\\left( {10} \\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code this into a small function\n",
    "def bounded_gutenberg_richter(m, a, b, mmin, mmax):\n",
    "    rate_m0 = 10.0 ** (a - b * mmin)\n",
    "    beta = b * np.log(10.)\n",
    "    rate = rate_m0 * (np.exp(-beta * (m - mmin)) - np.exp(-beta * (mmax - mmin))) /\\\n",
    "        (1.0 - np.exp(-beta * (mmax - mmin)))\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum magnitude ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation (estimation, really) of the maximum magnitude is a whole other story!\n",
    "\n",
    "For the time being, and for your projects, you could:\n",
    "\n",
    "1. Just use the largest in your catalogue (obs. Mmax)\n",
    "\n",
    "2. Use the largest in the catalogue plus an additional increment (e.g. obs. Mmax + 0.5, + 1.0 etc.)\n",
    "\n",
    "3. Find out the largest event to have occurred in your region?\n",
    "\n",
    "4. Find out the largest event to have occurred in tectonic environments similar to yours?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum observed\n",
    "mmax_obs = np.max(magnitudes)\n",
    "mmax = mmax_obs + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.semilogy(midpoints, cum_rates, \"o\")\n",
    "plt.semilogy(midpoints, 10.0 ** (awc - bwc * midpoints), \"k-\", lw=2, label=\"Weichert\")\n",
    "plt.semilogy(midpoints, 10.0 ** (als - bls * midpoints), \"r-\", lw=2, label=\"Linear LSQ.\")\n",
    "reference_magnitudes = np.arange(3., 7.2, 0.05)\n",
    "model_cum_rates = bounded_gutenberg_richter(reference_magnitudes,\n",
    "                                            awc, bwc, 3.0, mmax)\n",
    "plt.semilogy(reference_magnitudes, model_cum_rates, \"b--\", lw=2, label=\"Bounded GR\\nfrom Weichert\")\n",
    "plt.legend(loc=3)\n",
    "plt.grid()\n",
    "plt.xlabel(\"M\", fontsize=14)\n",
    "plt.ylabel(r\"$N_C$\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally ... can we answer question 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of observing this earthquake (e.g. M > m) in a 50 year period?\n",
    "\n",
    "The Gutenberg-Richter model (bounded or unbounded) can tell us the annual rate of events exceeding a given magnitude. But how can we use this to answer the question of the probability of an event in a given time period, T?\n",
    "\n",
    "For this we use the Poisson model\n",
    "\n",
    "$P\\left( {M > m | T} \\right) = 1.0 - \\exp\\left( {-\\lambda T} \\right)$\n",
    "\n",
    "where $\\lambda$ is the annual rate of events with $M > m$ and T is the time period.\n",
    "\n",
    "So, from the model above, what is the probability of experiencing an earthquake with magnitude greater than or equal to M 6.0 in a 10 year time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the bounded Gutenberg-Richter\n",
    "rate_mgt6 = bounded_gutenberg_richter(6, awc, bwc, 3.0, mmax)\n",
    "print(\"The rate of evetns with M >= 6 is %.4f\" % rate_mgt6)\n",
    "\n",
    "prob_10yrs = 1.0 - np.exp(-rate_mgt6 * 10.0)\n",
    "print(\"The probability of an event with M >= 6 in 10 years is %.4f\" % prob_10yrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
